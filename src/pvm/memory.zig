const std = @import("std");
const Allocator = std.mem.Allocator;
const trace = @import("../tracing.zig").scoped(.pvm);

pub const Memory = struct {
    pub const Page = struct {
        data: []u8,
        address: u32,
        flags: Flags,

        const Size = Memory.Z_P;

        pub const Flags = enum {
            ReadOnly,
            ReadWrite,
        };

        pub fn init(allocator: Allocator, address: u32, flags: Flags) !Page {
            const data = try allocator.alloc(u8, Memory.Z_P);
            @memset(data, 0);
            return Page{
                .data = data,
                .address = address,
                .flags = flags,
            };
        }

        pub fn deinit(self: *Page, allocator: Allocator) void {
            allocator.free(self.data);
            self.* = undefined;
        }
    };

    pub const PageTable = struct {
        pages: std.ArrayList(Page),
        allocator: Allocator,

        pub fn init(allocator: Allocator) PageTable {
            return .{
                .pages = std.ArrayList(Page).init(allocator),
                .allocator = allocator,
            };
        }

        pub fn deinit(self: *PageTable) void {
            for (self.pages.items) |*page| {
                page.deinit(self.allocator);
            }
            self.pages.deinit();
            self.* = undefined;
        }

        /// Allocates a contiguous range of pages starting at the given address
        pub fn allocatePages(self: *PageTable, start_address: u32, num_pages: usize, flags: Page.Flags) !void {
            const span = trace.span(.allocate_pages);
            defer span.deinit();
            span.debug("Allocating {d} {s} page(s) starting at 0x{X:0>8}", .{ num_pages, @tagName(flags), start_address });

            // Check for overlapping pages
            for (self.pages.items) |page| {
                const new_end = start_address + (num_pages * Memory.Z_P);
                const page_end = page.address + Memory.Z_P;

                if ((start_address >= page.address and start_address < page_end) or
                    (new_end > page.address and new_end <= page_end))
                {
                    span.err("Page allocation would overlap existing pages", .{});
                    return error.PageOverlap;
                }
            }

            // Allocate new pages
            var i: usize = 0;
            while (i < num_pages) : (i += 1) {
                const page_addr: u32 = start_address + (@as(u32, @intCast(i)) * Memory.Z_P);
                const page = try Page.init(self.allocator, page_addr, flags);
                span.debug("Created page at 0x{X:0>8} with flags {s}", .{ page.address, @tagName(page.flags) });
                try self.pages.append(page);
            }

            // Sort pages by address
            std.sort.insertion(Page, self.pages.items, {}, struct {
                fn lessThan(_: void, a: Page, b: Page) bool {
                    return a.address < b.address;
                }
            }.lessThan);
        }

        /// Returns the index of the page containing the given address using binary search
        pub fn findPageIndexOfAddress(self: *const PageTable, address: u32) ?usize {
            var left: usize = 0;
            var right: usize = self.pages.items.len;

            while (left < right) {
                const mid = left + (right - left) / 2;
                const page = self.pages.items[mid];

                if (page.address <= address and address < page.address + Page.Size) {
                    return mid;
                } else if (page.address < address) {
                    left = mid + 1;
                } else {
                    right = mid;
                }
            }
            return null;
        }

        pub const PageResult = struct {
            page: *Page,
            index: usize,
            page_table: *PageTable,

            pub fn next(self: PageResult) ?PageResult {
                if (self.index + 1 >= self.page_table.pages.items.len) return null;
                return PageResult{
                    .page = &self.page_table.pages.items[self.index + 1],
                    .index = self.index + 1,
                    .page_table = self.page_table,
                };
            }

            pub fn nextContiguous(self: PageResult) ?PageResult {
                if (self.next()) |next_result| {
                    const expected_address = self.page.address + Page.Size;
                    if (next_result.page.address == expected_address) {
                        return next_result;
                    }
                }
                return null;
            }
        };

        pub fn findPageOfAddresss(self: *PageTable, address: u32) ?PageResult {
            if (self.findPageIndexOfAddress(address)) |index| {
                return PageResult{
                    .page = &self.pages.items[index],
                    .index = index,
                    .page_table = self,
                };
            }
            return null;
        }

        const OrdPages = struct {
            fn order(ctx: *const Page, item: *const Page) std.math.Order {
                return std.math.order(ctx.address, item.address);
            }
        }.order;
    };

    page_table: PageTable,
    last_violation: ?ViolationInfo,
    allocator: Allocator,
    input_size_in_bytes: u32,
    read_only_size_in_pages: u16,
    stack_size_in_pages: u16,
    heap_size_in_pages: u16,

    // Artificial limit on how many allocations we allow
    // mainly for the fuzzer, this is not in the spec
    heap_allocation_limit: ?u16 = null,

    // Memory layout constants
    pub const Z_Z: u32 = 0x10000; // 2^16 = 65,536 - Major zone size
    pub const Z_P: u32 = 0x1000; // 2^12 = 4,096 - Page size
    pub const Z_I: u32 = 0x1000000; // 2^24 - Standard input data size

    // Fixed section base addresses
    pub const READ_ONLY_BASE_ADDRESS: u32 = Z_Z;
    pub fn HEAP_BASE_ADDRESS(read_only_size_in_bytes: usize) !u32 {
        return 2 * Z_Z + @as(u32, @intCast(try alignToSectionSize(read_only_size_in_bytes)));
    }
    pub const INPUT_ADDRESS: u32 = 0xFFFFFFFF - Z_Z - Z_I;
    pub const STACK_BASE_ADDRESS: u32 = 0xFFFFFFFF - (2 * Z_Z) - Z_I;
    pub fn STACK_BOTTOM_ADDRESS(stack_size_in_pages: u16) !u32 {
        return STACK_BASE_ADDRESS - @as(u32, @intCast(try alignToPageSize(stack_size_in_pages)));
    }

    pub const ViolationType = enum {
        WriteProtection,
        AccessViolation,
        NonAllocated,
    };

    pub const ViolationInfo = struct {
        violation_type: ViolationType,
        address: u32, // aligned to page
        attempted_size: usize,
        page: ?*Page = null,
    };

    pub const Error = error{
        PageFault,
        CrossPageWrite,
        CrossPageRead,
        OutOfMemory,
        CouldNotFindRwPage,
        MemoryLimitExceeded,
    };

    /// Aligns size to the next page boundary (Z_P = 4096)
    fn alignToPageSize(size_in_bytes: usize) !u32 {
        return try sizeInBytesToPages(size_in_bytes) * Z_P;
    }

    /// Does a bytes to pages
    fn sizeInBytesToPages(size: usize) !u16 {
        return @intCast(try std.math.divCeil(@TypeOf(size), size, Z_P));
    }

    /// does a pages to bytes
    fn pagesToSizeInBytes(pages: u16) usize {
        return pages * Z_P;
    }

    /// Aligns size to the next section boundary (Z_Z = 65536)
    fn alignToSectionSize(size_in_bytes: usize) !u32 {
        return @as(u32, @intCast(try std.math.divCeil(@TypeOf(size_in_bytes), size_in_bytes, Z_Z))) * Z_Z;
    }

    pub fn isMemoryError(err: anyerror) bool {
        return err == Error.PageFault;
    }

    fn checkMemoryLimits(ro_size_in_bytes: usize, heap_size_in_bytes: usize, stack_size_in_bytes: usize) !void {
        // Calculate sizes in terms of major zones (Z_Z)
        const ro_zones = try alignToSectionSize(ro_size_in_bytes);
        const heap_zones = try alignToSectionSize(heap_size_in_bytes);
        const stack_zones = try alignToSectionSize(stack_size_in_bytes);

        // Check the memory layout equation: 5Z_Z + ⌈|o|/Z_Z⌉ + ⌈|w|/Z_Z⌉ + ⌈s/Z_Z⌉ + Z_I ≤ 2^32
        var total: u64 = 5 * Z_Z; // Fixed zones
        total += ro_zones;
        total += heap_zones;
        total += stack_zones;
        total += Z_I; // Input data section

        if (total > 0xFFFFFFFF) {
            return Error.MemoryLimitExceeded;
        }
    }

    pub fn initEmpty(allocator: Allocator) !Memory {
        // Initialize page table
        var page_table = PageTable.init(allocator);
        errdefer page_table.deinit();

        return Memory{
            .page_table = page_table,
            .last_violation = null,
            .allocator = allocator,
            .input_size_in_bytes = 0,
            .read_only_size_in_pages = 0,
            .stack_size_in_pages = 0,
            .heap_size_in_pages = 0,
        };
    }

    pub fn initWithCapacity(
        allocator: Allocator,
        read_only_size_in_bytes: usize,
        heap_size_in_pages: u16,
        input_size_in_bytes: usize,
        stack_size_in_bytes: usize,
    ) !Memory {
        const span = trace.span(.memory_init);
        defer span.deinit();
        span.debug("Starting memory initialization", .{});

        // Initialize page table
        var page_table = PageTable.init(allocator);
        errdefer page_table.deinit();

        // Calculate section sizes
        const read_only_aligned_size_in_bytes = try alignToPageSize(read_only_size_in_bytes);
        const heap_aligned_size_in_bytes = @as(usize, heap_size_in_pages * Z_P);
        const stack_aligned_size_in_bytes = try alignToPageSize(stack_size_in_bytes);

        // Verify memory limits
        try checkMemoryLimits(
            read_only_aligned_size_in_bytes,
            heap_aligned_size_in_bytes,
            stack_aligned_size_in_bytes,
        );

        // Allocate the ReadOnly section
        try page_table.allocatePages(
            READ_ONLY_BASE_ADDRESS,
            try sizeInBytesToPages(read_only_size_in_bytes),
            .ReadOnly,
        );

        // Allocate the Heap section
        const heap_base = try HEAP_BASE_ADDRESS(read_only_aligned_size_in_bytes);
        try page_table.allocatePages(heap_base, heap_size_in_pages, .ReadWrite);

        // Allocate the Input section
        try page_table.allocatePages(INPUT_ADDRESS, try sizeInBytesToPages(input_size_in_bytes), .ReadOnly);

        // Allocate the Stack section
        const stack_size_in_pages = try sizeInBytesToPages(stack_aligned_size_in_bytes);
        try page_table.allocatePages(try STACK_BOTTOM_ADDRESS(stack_size_in_pages), stack_size_in_pages, .ReadWrite);

        return Memory{
            .page_table = page_table,
            .last_violation = null,
            .allocator = allocator,
            .input_size_in_bytes = @as(u32, @intCast(input_size_in_bytes)),
            .read_only_size_in_pages = try sizeInBytesToPages(read_only_size_in_bytes),
            .stack_size_in_pages = stack_size_in_pages,
            .heap_size_in_pages = heap_size_in_pages,
        };
    }

    pub fn initWithData(
        allocator: Allocator,
        read_only: []const u8,
        read_write: []const u8,
        input: []const u8,
        stack_size_in_bytes: u24,
        heap_size_in_pages: u16,
    ) !Memory {
        const span = trace.span(.memory_init);
        defer span.deinit();

        span.debug("Initializing memory system with data", .{});

        // Calculate section sizes rounded to page boundaries
        const ro_pages: u16 = @intCast(try sizeInBytesToPages(read_only.len));
        const heap_pages: u16 = @intCast(heap_size_in_pages + try sizeInBytesToPages(read_write.len));

        // Initialize the memory system with the calculated capacities
        var memory = try Memory.initWithCapacity(
            allocator,
            ro_pages,
            heap_pages,
            input.len,
            stack_size_in_bytes,
        );
        errdefer memory.deinit();

        // Initialize read-only section
        if (read_only.len > 0) {
            try memory.initMemory(READ_ONLY_BASE_ADDRESS, read_only);
        }

        // Initialize heap section with read-write data
        if (read_write.len > 0) {
            const heap_base = try HEAP_BASE_ADDRESS(@intCast(ro_pages * Z_P));
            try memory.initMemory(heap_base, read_write);
        }

        // Initialize input section
        if (input.len > 0) {
            try memory.initMemory(INPUT_ADDRESS, input);
        }

        // Stack is already zero-initialized by initWithCapacity

        span.debug("Memory initialization complete", .{});
        return memory;
    }

    /// Allocate a single page at a specific address
    pub fn allocatePageAt(self: *Memory, address: u32, flags: Page.Flags) !void {
        return self.allocatePagesAt(address, 1, flags);
    }

    /// Allocate multiple contiguous pages starting at a specific address
    /// Address must be page aligned
    pub fn allocatePagesAt(self: *Memory, address: u32, num_pages: usize, flags: Page.Flags) !void {
        const span = trace.span(.memory_allocate_pages_at);
        defer span.deinit();

        // Ensure address is page aligned
        if (address % Z_P != 0) {
            return error.UnalignedAddress;
        }

        // Allocate the new pages
        try self.page_table.allocatePages(address, num_pages, flags);
    }

    pub fn allocate(self: *Memory, memory_requested: u32) !u32 {
        const span = trace.span(.memory_allocate);
        defer span.deinit();

        if (self.heap_allocation_limit) |limit| {
            if (self.heap_size_in_pages + try sizeInBytesToPages(memory_requested) > limit)
                return error.MemoryLimitExceeded;
        }

        // Calculate required pages, rounding up to nearest page size
        const aligned_size = try alignToPageSize(memory_requested);
        const pages_needed = aligned_size / Z_P;

        // Check if the size is within bounds
        try checkMemoryLimits(
            pagesToSizeInBytes(self.read_only_size_in_pages),
            pagesToSizeInBytes(self.heap_size_in_pages),
            pagesToSizeInBytes(self.stack_size_in_pages),
        );

        // TODO: > HEAP_BASE_ADDRESS should start at start of heap page section

        // Find the last ReadWrite page (heap section)
        var last_rw_page: ?PageTable.PageResult = null;
        for (self.page_table.pages.items, 0..) |*page, i| {
            if (page.flags == .ReadWrite) {
                last_rw_page = PageTable.PageResult{
                    .page = page,
                    .index = i,
                    .page_table = &self.page_table,
                };
            }
        }

        const last_page = last_rw_page orelse {
            return Error.CouldNotFindRwPage;
        };

        // Calculate new allocation address (immediately after last ReadWrite page)
        const new_address = last_page.page.address + Z_P;

        // Special case when size is 0, we are nog going to allocate
        // anything, we will return the address of the next page
        if (memory_requested == 0) {
            return new_address;
        }

        // Allocate the new pages
        try self.page_table.allocatePages(new_address, pages_needed, .ReadWrite);

        // Do some bookkeeping
        self.heap_size_in_pages += 1;

        return new_address;
    }

    /// Read an integer type from memory (u8, u16, u32, u64)
    /// Read any integer type and convert it to u64, handling sign extension for signed types
    pub fn readIntAndSignExtend(self: *Memory, comptime T: type, address: u32) !u64 {
        const value = try self.readInt(T, address);
        return switch (@typeInfo(T)) {
            .int => |info| switch (info.signedness) {
                .signed => @bitCast(@as(i64, @intCast(value))),
                .unsigned => value,
            },
            else => @compileError("Only integer types are supported"),
        };
    }

    /// Read an integer type from memory (u8, u16, u32, u64)
    pub fn readInt(self: *Memory, comptime T: type, address: u32) !T {
        const span = trace.span(.memory_read);
        defer span.deinit();

        const size = @sizeOf(T);
        comptime std.debug.assert(size <= 8); // Only handle up to u64

        // Get first page and offset
        const first_page = self.page_table.findPageOfAddresss(address) orelse {
            self.last_violation = ViolationInfo{
                .violation_type = .NonAllocated,
                .address = @divTrunc(address, Z_P) * Z_P,
                .attempted_size = @sizeOf(T),
                .page = null,
            };
            return Error.PageFault;
        };
        const offset = address - first_page.page.address;
        const bytes_in_first = Memory.Z_P - offset;

        // If it fits in first page, read directly
        if (size <= bytes_in_first) {
            return @as(T, @bitCast(first_page.page.data[offset..][0..size].*));
        }

        // Cross-page read - use stack buffer
        var buf: [@sizeOf(T)]u8 = undefined;

        // Copy from first page
        @memcpy(buf[0..bytes_in_first], first_page.page.data[offset..][0..bytes_in_first]);

        // Get and copy from second page, this should aways have enough
        const next_page = first_page.nextContiguous() orelse {
            self.last_violation = ViolationInfo{
                .violation_type = .NonAllocated,
                .address = first_page.page.address + Memory.Z_P, // Address where next page should be
                .attempted_size = size,
                .page = first_page.page,
            };
            return error.PageFault;
        };
        const bytes_in_second = size - bytes_in_first;
        @memcpy(buf[bytes_in_first..size], next_page.page.data[0..bytes_in_second]);

        return std.mem.readInt(T, &buf, .little);
    }

    /// Read a slice from memory, not allowing cross-page reads
    pub fn readSlice(self: *Memory, address: u32, size: usize) ![]const u8 {
        const span = trace.span(.memory_read);
        defer span.deinit();

        // Find the page containing the address
        const page = self.page_table.findPageOfAddresss(address) orelse {
            self.last_violation = ViolationInfo{
                .violation_type = .NonAllocated,
                .address = address,
                .attempted_size = size,
                .page = null,
            };
            return Error.PageFault;
        };

        // Check if read would cross page boundary
        const offset = address - page.page.address;
        if (offset + size > Z_P) {
            return Error.CrossPageRead;
        }

        // Return slice of page data
        return page.page.data[offset..][0..size];
    }

    /// Initilialize a slice to memory not cross-page, will err on cross page
    /// access. Does not check ReadWrite permissions can also write to ReadOnly
    /// segments.
    pub fn initMemory(self: *Memory, address: u32, slice: []const u8) !void {
        const span = trace.span(.memory_write);
        defer span.deinit();

        if (slice.len == 0) return;

        var remaining = slice;
        var current_addr = address;

        while (remaining.len > 0) {
            // Find the page containing the current address
            const page_result = self.page_table.findPageOfAddresss(current_addr) orelse {
                return Error.PageFault;
            };

            // Calculate offset and available space in current page
            const offset = current_addr - page_result.page.address;
            const available_in_page = Z_P - offset;
            const bytes_to_write = @min(remaining.len, available_in_page);

            // Write data to current page
            @memcpy(page_result.page.data[offset..][0..bytes_to_write], remaining[0..bytes_to_write]);

            // If we need to continue to next page, verify it exists and is contiguous
            if (remaining.len > bytes_to_write) {
                _ = page_result.nextContiguous() orelse {
                    return Error.PageFault;
                };
            }

            // Update pointers for next iteration
            remaining = remaining[bytes_to_write..];
            current_addr += bytes_to_write;
        }
    }

    /// Write an integer type to memory (u8, u16, u32, u64)
    pub fn writeInt(self: *Memory, T: type, address: u32, value: T) !void {
        const span = trace.span(.memory_write);
        defer span.deinit();

        const size = @sizeOf(T);
        comptime std.debug.assert(size <= 8); // Only handle up to u64
        comptime std.debug.assert(@typeInfo(T) == .int);

        // First verify all required pages exist
        const page = self.page_table.findPageOfAddresss(address) orelse {
            self.last_violation = ViolationInfo{
                .violation_type = .NonAllocated,
                .address = @divTrunc(address, Z_P) * Z_P, // since this falls outsize any page, we just round ti
                .attempted_size = size,
                .page = null,
            };
            return Error.PageFault;
        };

        const next_contiguous = if (size > (page.page.address + Z_P) - address)
            page.nextContiguous() orelse {
                self.last_violation = ViolationInfo{
                    .violation_type = .NonAllocated,
                    .address = page.page.address + Z_P,
                    .attempted_size = size,
                    .page = null,
                };
                return Error.PageFault;
            }
        else
            null;

        // TODO: Check if both pages are ReadWrite

        // Now perform the actual write, knowing all pages exist
        var bytes: [size]u8 = undefined;
        std.mem.writeInt(T, &bytes, value, .little);

        // Write to first page
        const offset = address - page.page.address;
        const bytes_in_first = Z_P - offset;
        const first_write_size = @min(size, bytes_in_first);
        @memcpy(page.page.data[offset..][0..first_write_size], bytes[0..first_write_size]);

        // Write to second page if needed
        if (next_contiguous) |_| {
            const bytes_in_second = size - bytes_in_first;
            @memcpy(next_contiguous.?.page.data[0..bytes_in_second], bytes[bytes_in_first..size]);
        }
    }

    // Helper methods for common types
    pub fn readU8(self: *Memory, address: u32) !u8 {
        return self.readInt(address, u8);
    }

    pub fn readU16(self: *Memory, address: u32) !u16 {
        return self.readInt(address, u16);
    }

    pub fn readU32(self: *Memory, address: u32) !u32 {
        return self.readInt(address, u32);
    }

    pub fn readU64(self: *Memory, address: u32) !u64 {
        return self.readInt(address, u64);
    }

    pub fn writeU8(self: *Memory, address: u32, value: u8) !void {
        return self.writeInt(address, value);
    }

    pub fn writeU16(self: *Memory, address: u32, value: u16) !void {
        return self.writeInt(address, value);
    }

    pub fn writeU32(self: *Memory, address: u32, value: u32) !void {
        return self.writeInt(address, value);
    }

    pub fn writeU64(self: *Memory, address: u32, value: u64) !void {
        return self.writeInt(address, value);
    }

    pub fn deinit(self: *Memory) void {
        const span = trace.span(.memory_deinit);
        defer span.deinit();

        self.page_table.deinit();
        self.* = undefined;
    }

    pub fn getLastViolation(self: *const Memory) ?ViolationInfo {
        return self.last_violation;
    }
};
